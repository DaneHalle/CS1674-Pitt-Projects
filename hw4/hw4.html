<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>CS1674: Homework 4 </title>
</head>
<body>
<h2>CS1674: Homework 4 </h2>
<b>Due:</b> 3/2/2021, 11:59pm
<br><br>

This assignment is worth 50 points.
<br><br>
<b>Download HW4_starter.zip from the Canvas HW4 Submission Page. (similar to how you got hw2_starter.zip for HW2)</b>
<br><br><br>

<u>Part I: Feature Description</u> (30 points)
<br><br>
In this problem, you will implement a feature description pipeline, as discussed in class. While you will not exactly implement that, <a href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">the SIFT paper</a> by David Lowe is a useful resource, in addition to Section 4.1 of the Szeliski textbook.
<br><br>
Use the following signature: <font face="courier new">function [features] = compute_features(x, y, scores, Ix, Iy)</font> (note <font face="courier new">Ix=Ih, Iy=Iv</font>). The inputs <font face="courier new">x, y, scores, Ix, Iy </font> are defined in HW3. The output <font face="courier new">features</font> is an <i>N</i>x<i>d</i> matrix, each row of which contains the <i>d</i>-dimensional descriptor for the <i>n</i>-th keypoint.
We'll simplify the histogram creation procedure a bit, compared to the original implementation presented in class. In particular, we'll compute a descriptor with dimensionality <i>d</i>=8 (rather than 4x4x8), which contains an 8-dimensional histogram of gradients computed from a 11x11 grid centered around each detected keypoint (i.e. -5:+5 neighborhood horizontally and vertically).
<ol>
<li>[4 pts] If any of your detected keypoints are less than 5 pixels from the top/left or 5 pixels from the bottom/right of the image, i.e. pixels lacking 5+5 neighbors in either the horizontal or vertical direction, erase this keypoint from the <font face="courier new">x, y, scores</font> vectors <i>at the start of your code</i> and do not compute a descriptor for it.</li>
<li>[8 pts] To compute the gradient magnitude <i>m(x, y)</i> and gradient angle <i>Î¸(x, y)</i> at point (x, y), take <i>I</i> to be the image and use the formula below shown in class and Matlab's <font face="courier new">atand</font>, which returns values in the range [-90, 90]. If the gradient magnitude is 0, then both the <i>x</i> and <i>y</i> gradients are 0, and you should ignore the orientation for that pixel (since it won't contribute to the histogram).
</br><img src="./CS1674_ Homework 4_files/gradients2.png"></br>
Note that <font face="courier new">Ix, Iy</font> can be derived the usual way ('prewitt', 'sobel', etc.) following however you computed them in <font face="courier new">extract_keypoints</font> from HW3.
<br><br>
</li><li>[6 pts] Quantize the gradient orientations in 8 bins (so put values between -90 and -67.5 degrees in one bin, the -67.5 to -45 degree angles in another bin, etc.) For example, you can have a quantization matrix with the same size as the image, that says to which bin (1 through 8) the gradient of each pixel belongs.</li>
<li>[6 pts] Now, let's compute the SIFT histogram for each keypoint (based on its 11x11 grid centered around each detected keypoint, i.e. -5:+5 neighborhood horizontally and vertically). To populate the SIFT histogram, consider each of the 8 bins. To populate the first bin, sum the gradient magnitudes that are between -90 and -67.5 degrees. Repeat analogously for all bins. </li>
<li>[6 pts] Finally, you should clip all values to 0.2 as discussed in class, and normalize each descriptor to be of unit length, e.g. using <font face="courier new">hist_final = hist_final / sum(hist_final);</font> Normalize both before and after the clipping (i.e., 1. normalize, 2. clip, 3. normalize again). You do not have to implement any more sophisticated detail from the Lowe paper. </li>

</ol>
<br>


<u>Part II: Image Description with SIFT Bag-of-Words</u> (10 points)
<br><br>

In this part, you will compute a bag-of-words histogram representation of an image. Conceptually, the histogram for image
<i>I<sub>j</sub></i> is a <i>k</i>-dimensional vector: <i>F(I<sub>j</sub>) = [&nbsp;freq<sub>1, j</sub> &nbsp;&nbsp; freq<sub>2, j</sub> &nbsp;&nbsp; ... &nbsp;&nbsp; freq<sub>k, j</sub>&nbsp;]</i>, where each entry
<i>freq<sub>i, j</sub></i> counts the number of occurrences of the <i>i</i>-th visual word in image <i>j</i>, and <i>k</i> is
the number of total words in the vocabulary. (Acknowledgement: Notation from Kristen Grauman's assignment.)

<br><br>
Use the following function signature: <font face="courier new">function [bow_repr] = computeBOWRepr(features, means)</font> where <font face="courier new">bow_repr</font> is a normalized bag-of-words histogram (a <i>row</i> vector), <font face="courier new">features</font> is the <i>N</i>x8 set of descriptors computed for the image (output by the function you implemented in Part I above), and <font face="courier new">means</font> is a <i>k</i>x8 set of cluster means. A set of files with cluster means are provided for you on Canvas (for <i>k</i>={2, 5, 10, 50, 100, 200}).

<ol>
<li>[2 pt] A bag-of-words histogram has as many dimensions as the number of clusters <i>k</i>, so initialize the <font face="courier new">bow</font> variable accordingly.</li>
<li>[4 pts] Next, for each feature (i.e. each row in <font face="courier new">features</font>), compute its distance to each of the cluster means, and find the closest mean. A feature is thus conceptually "mapped" to the closest cluster. You can do this efficiently using Matlab's <font face="courier new">pdist2</font> function (with inputs <font face="courier new">features, means</font>).</li>
<li>[4 pts] To compute the bag-of-words histogram, count how many features are mapped to each cluster.</li>
<li>[2 pts] Finally, normalize the histogram by dividing each entry by the sum of the entries. (just a simple normalization. No clipping.)</li>
</ol>
<br>


<u>Part III: Comparison of Image Descriptors</u> (10 points)
<br><br>

In this part, we will test the quality of the different representations. A good representation is one that retains some of the semantics of the image; oftentimes by "semantics" we mean object class label. In other words, a good representation should be one such that two images of the same object have similar representations, and images of different objects have different representations. We will test to what extent this is true, using our images of cardinals, leopards and pandas.
<br><br>
To test the quality of the representations, we will compare two averages: the average <i>within-class distance</i> and the average <i>between-class distance</i>. A representation is a vector, and "distance" is the Euclidean distance between two vectors (i.e. the representations of two images). "Within-class distances" are distances computed between the vectors for images of the same class (i.e. leopard-leopard, panda-panda). "Between-class distances" are those computed between images of different classes, e.g. leopard-panda. If you have a good image representation, should the average within-class or the average between-class distance be smaller?
<br><br>
A script <font face="courier new">compare_representations.m</font> is provided for you, which does the following:
<ol>
<li>Reads the images, and resizes them to 300x300.</li>
<li>Uses the (a) code you wrote above, (b) the function <font face="courier new">extract_keypoints</font> you wrote in HW3 (talk to the TA or instructor if that function doesn't work), and (c) another <font face="courier new">function [texture_repr_concat, texture_repr_mean] = computeTextureReprs(image, F)</font> provided for you, to compute three types image representations (bag of words using SIFT descriptors using different values of <i>k</i> i.e. different number of cluster centers, concatenation of the texture representation of all pixels, and average over the texture representation of pixels) for each image.</li>
<li>Computes and prints the ratio <i>average_within_class_distance / average_between_class_distance</i> for each representation type. </li>
<li><font face="courier new">compare_represenations</font> calls <font face="courier new">computeTextureReprs</font> which has the following inputs: <font <font="" face="courier new">image</font> is the output of an <font face="courier new">imread</font>, and <font face="courier new">F</font> is the 49x49x<i>num_filters</i> matrix of filters you used in HW2. The latter function computes two image representations based on the filter responses. The first is simply a concatenation of the filter responses for all pixels and all filters. The second contains the mean of filter reponses (averaged across all pixels) for each image.  </li>
</ol>
Your task in this part is to do the following. Run <font face="courier new">compare_representations</font> (check the top of the script for use tips); it tries different values of <i>k</i>. Experiment with different values for the number of keypoints you extract from images, by changing your <font face="courier new">extract_keypoints</font> function (no need to submit any changes to it; we'll use our own function). Then, in a file <font face="courier new">answers.txt</font>, answer the following questions.
<ul>
<li>For which of the three representations is the within-between ratio smallest? </li>
<li>Does the answer to this question depend on the value of <i>k</i> that you use? </li>
<li>Does it depend on the number of keypoints you extract? (Try 500, 1000, 2000, 3000.) </li>
<li>Which of the three types of descriptors that you used is the best one? How can you tell? </li>
<li>Is this what you expected? Why or why not? </li>
</ul>
You can copy-paste ratios from the output of <font face="courier new">compare_representations</font> as needed to support your answers.
<br>
Notes:
<ol>
<li>Do not worry if you see NaN for <font face="courier new">k=2</font>.</li>
<li>Do not worry if the patterns are not obvious. They may not be strictly increasing or decreasing.</li>
<li>You would expect the within-between ratio to be <font face="courier new"> <1 </font> for some <font face="courier new">k</font>, since that implies the within-class (i.e., images of same animals) distances are smaller than the between-class (i.e., images of different animals). Thus, checking to see if the within-between ratio is <font face="courier new"> <1 </font> for some <font face="courier new">k</font> would be the least sanity check.</li>
<li>The purpose of Part III is to use the provided script to make observations. This is similar to an actual research experiment. Depending on implementation of your compute_features.m, extract_keypoints.m, and the cluster centers, your answers may differ.
  We are looking for your descriptive observations based on the questions above. We are <b>not</b> looking to see if you provide the correct <i>k</i>, the number of keypoints, etc., because they depend on many factors.</li>
</ol>
<br><br><br>

<b>Submission:</b>
<ul>
<li><font face="courier new">compute_features.m</font> (from Part I)</li>
<li><font face="courier new">computeBOWRepr.m</font> (from Part II)</li>
<li><font face="courier new">answers.txt</font> (from Part III)</li>
</ul>
<br>


<b>Acknowledgement:</b> Adriana Kovashka.
</body></html>
